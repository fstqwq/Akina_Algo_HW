\documentclass[11pt,a4paper,oneside]{article}

\usepackage{euler,amsthm,amsmath,amsfonts,graphicx,epigraph,indentfirst,enumerate,comment,listings,fontspec,color,subcaption,listings}
\usepackage{xeCJK}
\usepackage{hw}
\usepackage{pythonhighlight}

\renewcommand{\hwtitle} {CS217 Homework 1}
\renewcommand{\hwauthor}{Akina}
\renewcommand{\hwdate}{\today}

\begin{document}
\title{\hwtitle}
\author{\hwauthor}
\date{\hwdate}
\maketitle

\section*{Bit Complexity of Euclid's Algorithm}
We have proved that Euclid's algorithm for computing $\gcd(a,b)$ makes at most
$O(\log a)$ iterations. What is the overall running time? Each iteration computes
$u \mod v$ for some integers. This can be done by integer division. What is its running time?
There are very sophisticated algorithms, but python probably does not come with them. 
Recall the ``school method'' for dividing integers. Have a look at the pdf slides on the 
webpage for an illustration of the school method. It is especially simple if we are dealing
with binary numbers. If $a$ and $b$ have at most $n$ bits, then the school method 
has complexity $O(n^2)$.

\begin{problem}{1}
	\statement
	Show the following, more precise bound of the school method for integer division:
	If $a$ has $n$ bits and $b$ has $k$ bits, then the school method can be implemented
	to run in $O( k(n-k))$ operations.
	\solution
	\begin{proof}
	School method for integer division will do $O( n - k )$ comparisons and subtractions. For each comparisons and subtractions, there are  $O(k)$ bits involved in the arithmetic. Hence the complexity of the algorithm is \( O(k(n - k))\).

	We can check our result by investigating the worst case in which the division answer like $(11\cdots1)_2$. It needs $(n - k + 1)$ fully-compared comparisons and subtractions, for each comparison it involves $k$ bits, for each subtractions it involves at most $k + 1$ bits. So the in total the bit complexity is $(n - k)(k + k + 1)$, which reaches the bound of our former result.

	\end{proof}
\end{problem}

\begin{problem}{2}
\statement
Show that the bit complexity of Euclid's algorithm, using the school method
to compute $a \mod b$, is $O(n^2)$. That is,
if $a$ and $b$ have at most $n$ bits, then $\gcd(a,b)$ makes $O(n^2)$ bit operations.\\

In order to do so, here is python code of the Euclidean algorithm:
\begin{python}
def euclid(a,b):
	while (b > 0):
		r = a % b # so a = bu+r
		if (r == 0):
			return b
		s = b % r # so b = rv + s
		a = r
		b = s
  	return a
\end{python}
Don't be afraid to introduce notation! I recommend to let $n$ denote the number of bits of $a$.
Take some other letters for the number of bits in $b$ and so on.
\solution
\begin{proof}
	
We prove it by induction.

When \(n = 1\), if $a = 0$ or $b = 0$, the algorithm exit immediately thus $O(1)$, else then $a = 1$ and $b = 1$ there's only a division of two $1-$bit number, it's also $O(1)$. So for $n = 1$ the assumption is true.

Assume that we have proved it for all \(1 \leq n < k \), next we'll prove it for \( n=k\). By induction hypothesis, at least one of the two numbers has \(n \) bits. So we assume that the other number has \(m \) bits and we use \(T(k)\) as the complexity of the algorithm in the case of \(n=k\).

According to the conclusion we proved in Exercise 3.

\[
	\begin{split}
		T(k) &= O(m(k - m)) + T(m) \\
		&= O(m(k-m) + m^2)
	\end{split}
\]

\( O(m(k - m) + m^2) = O(k^2)\) for all \(m\) satisfying \(1 \leq m \leq k\).

Hence, the complexity of the algorithm is \(O(n^2)\).

\end{proof}
\end{problem}

\section*{Computing the Binomial Coefficient}

    Next, we will investigate the binomial coefficient ${n \choose k}$, which 
    you might also know by the notation $C^k_n$. The number ${n \choose k}$ is defined
    as the number of subsets of $\{1,\dots,n\}$ which have size exactly $k$. 
    This immediately shows that ${n \choose k}$ is $0$ if $k$ is negative or larger than $n$.
    You might have seen the following recurrence:
    \begin{align*}
     {n \choose k} & = {n-1 \choose k-1} + {n-1 \choose k} \textnormal{ if } n,k \geq 0 \ .
    \end{align*}



\begin{problem}{3} 
    \statement
        [A Recursive Algorithm for the Binomial Coefficient]
        Using pseudocode, write a recursive algorithm computing
        ${n \choose k}$. Implement it in python! What is 
        the running time of your algorithm, in terms of $n$ and $k$? Would you say it is an efficient
        algorithm? Why or why not?
    \solution
\begin{python}
# Recursive Algorithm for the Binomial Coefficien
def Rec(n, k):
    if k < 0 or k > n:
        return 0
    if n == 0:
	    return 1
    return Rec(n - 1, k - 1) + Rec(n - 1, k)
\end{python}
    
    To figure out the complexity of the recursive algorithm, what important is to figure out the number of recalculations for each sub-recursion.
    
    Notice that \( n \) always reduce by \( 1 \), and \( k \) always reduce by \(0\) or remain its value at each recursion.
    
    Hence sub-recursion Rec \( (p, q) \) will be recalculated \( {n - p \choose k - q} \) times
    
	\begin{lemma}
		\({n \choose k} \leq (\frac{en}{k})^k\)
	\begin{proof}
		Considering the taylor expansion of \(e^x\) for \(x = k\)
		
		\[
			e^k = \sum_{i=0}^{\infty} \frac{k^i}{i!}
		\]
		
		We just pick the term with respect to \(k\), then we directly get 
		
		\[
			e^k \leq \frac{k^i}{k!} \Rightarrow \frac{1}{k!} \leq (\frac{e}{k})^k
		\]
		
		Hence,
		
		\[
			\begin{split}
				{n \choose k} &= \frac{n(n-1)\dots(n-k+1)}{k!} \\
				&\leq \frac{n^k}{k!} \\
				&\leq (\frac{en}{k})^k
			\end{split}
		\]
		
    \end{proof}
    \end{lemma}
    Hence the complexity of the algorithm are as follows

    \[
	    \begin{split}
		    \sum_{i = 0}^{n} \sum_{j = 0}^{k} {n - i \choose k - j} \log{i \choose j}  &\leq \sum_{i = 0}^{n} \sum_{j = 0}^{k}  {n - i \choose k - j} k\log{\frac{en}{k}} \\
		    &= \sum_{i=0}^n 2^{n-i} k \log{\frac{en}{k}} \\
		    &= O(2^{n}k \log{\frac{en}{k}}) \\
		    &= O(2^{n}k \log{\frac{n}{k}})
	    \end{split}
    \]
\end{problem}


\begin{problem}{4}
    \statement
    [A Dynamic Programming Algorithm for the Binomial Coefficient]
    Using pseudocode, write a dynamic programming algorithm
    computing ${n \choose k}$. Implement it in python! What is it running time
    in terms of $n$ and $k$?
    Would you say your algorithm is efficient? Why or why not?
    \solution
\begin{python}
# Dynamic Programming Algorithm for the Binomial Coefficien
def DP(n,k):
	f=[[1 for i in range(0,k+1)] for j in range(0,n+1)]
	for i in range(1, n+1):
    	for j in range(1, k+1):
    		if (i <= j):
    			f[i][j] = 1
    		else:
    			f[i][j]=f[i-1][j]+f[i-1][j-1]
	return f[n][k]
\end{python}
    
    [TODO]:Add runtime list of exercise 3 and 4?
    
	For calculating ${n \choose k}$, the dynamic programming algorithm runs $n \times k$ iterations.
	
	For each iteration, if $f_{i-1,j}$ and $f_{i-1,j-1}$ have at most $m$ bits, then the running time of adding will be $O(m)$.
    
    According to the mathematical expression of ${n \choose k}=(\frac{en}{k})^k$, any element in f won't be longer then $k\log(\frac{en}{k})$ bits. If we take the upper bound of calculating f, which assumes every element in f is $k\log(\frac{en}{k})$ bits long, then the total running time will be $O(k^2n\log(\frac{n}{k}))$
    
    It's a rather better algorithm than the recursive one because it only calculate every ${n \choose k}$ once.

\end{problem}

\begin{problem}{5}
    \statement
    Suppose we are only interested in whether ${n \choose k}$ is even or odd,
  i.e., we want to compute ${n \choose k}  \mod 2$. You could do this by computing 
  ${n \choose k}$ using dynamic programming and then taking
  the result modulo $2$. What is the running time? Would you say this algorithm
  is efficient? Why or why not?
  \solution
If we first calculate ${n \choose k}$ in completely the same way of the previous task and take the last bit of the answer as the answer of this task, then the running time will still be the same as previous. It can't be a good algorithm because all we care is the last bit of the answer, but we use many time to calculate the useless higher bits.

For a better complexity, for each $f_{i, j}$ we only concern the lowest bit, so the complexity of multiplication and addition is $\Theta(1)$ because they are equivalent to bit AND and bit OR. In this case, the total complexity is $\Theta(nk)$.

However, we have a more efficient  -- $\Theta (\log n)$ -- conclusion:  $ {n \choose k} \bmod 2 = [n \wedge k = k]$.
	\begin{proof}
		According to the Lucas' Theorem, 
		
		\[
			{n \choose k}\equiv{{n \bmod p} \choose {k \bmod p}}{{ \lfloor n/p\rfloor} \choose { \lfloor k/p\rfloor}}(\bmod p)
		\]
		
		For $p=2$ and $n=(a_u....a_2a_1)_2$, $k=(b_v....b_2b_1)_2$, we apply Lucas' Theorem. Note that because $k\leq n$, we have $v\leq u$.
		
		\[
			\begin{split}
			{n \choose k} &\equiv {(a_u....a_2)_2 \choose (b_v....b_2)_2}{a_1 \choose b_1}\\
			&\equiv {(a_u....a_3)_2 \choose (b_v....b_3)_2}{a_2 \choose b_2}{a_1 \choose b_1}\\
			&\equiv ...\\
			&\equiv {(a_u....a_v+1)_2 \choose 0}{a_v \choose b_v}...{a_1 \choose b_1}\\
			&\equiv {a_v \choose b_v}...{a_1 \choose b_1}(\bmod 2)
			\end{split}
		\]
		
		Because $a_i,b_i\in \{0,1\}$ and ${0 \choose 0}={0 \choose 1}={1 \choose 1}=1$, ${0 \choose 1}=0$, we have:
		
		\[
			\begin{split}
			{n \choose k} \bmod 2 = 1 &\Leftrightarrow {a_v \choose b_v}...{a_1 \choose b_1}\bmod 2=1 \\
			&\Leftrightarrow \forall 1\leq i\ \leq v {a_i \choose b_i}=1\\
			&\Leftrightarrow \forall 1\leq i\ \leq v b_i=1\rightarrow a_i=1\\
			&\Leftrightarrow n \wedge k = k
			\end{split}
		\]		
    \end{proof}
\end{problem}

\begin{problem}{6}
    \statement
    Remember the ``period'' algorithm for computing $F'_n := (F_n \mod k)$ discussed in class:
    Find some $i,j$ between $0$ and $k^2$ for which 
    $F'_{i} =  F'_{j}$ and $F'_{i+1} = F'_{j+1}$. 
    Then for $d := j-i$ the sequence $F'_{n}$ will repeat every $d$ steps, as there will be a cycle.
    
    Show that a lasso cannot happen. That is, show 
    that the smallest $i$ for which this happens is $0$, i.e, for some $j$ we have
    $F'_0 = F'_j$ and $F'_1 = F'_{j+1}$ and thus $F'_n = F'_{n \text{ mod }  j}$.
    \solution
	\begin{proof}
	We prove this by contradiction.

	Suppose the smallest $i$ in some $(i, j)$ where $i < j$ that satisfies the above condition is greater than $0$.
	By definition of fibonacci number,
	$$F_{i-1} \equiv  F_{i+1}-F_i(\textnormal{ mod }  k)$$
	and similarly
	$$F_{j-1} \equiv  F_{j+1}-F_j(\textnormal{ mod }  k)$$
	
	Because $F_i = F_j$ and $F_{i+1} = F_{j+1}$, we have 
	$$F_{j-1} \equiv F_{i-1}(\textnormal{ mod }  k)$$
	and equivalently
	$$F'_{i-1} = F'_{j-1}$$

	We take $i^\prime = i - 1, j^\prime = j - 1$, then $(i^\prime, j ^ \prime)$ satisfies the preset condition. Notice that $i^\prime < i$, thus this leads to contradiction.
	\end{proof}
\end{problem}

\end{document}
