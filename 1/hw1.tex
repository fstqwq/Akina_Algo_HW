\documentclass[11pt,a4paper,oneside]{article}

\usepackage{euler,amsthm,amsmath,amsfonts,graphicx,epigraph,indentfirst,enumerate,comment,listings,fontspec,color,subcaption,listings}
\usepackage{xeCJK}
\usepackage{hw}
\usepackage{pythonhighlight}

\renewcommand{\hwtitle} {CS217 Homework 1}
\renewcommand{\hwauthor}{Akina}
\renewcommand{\hwdate}{\today}

\begin{document}
\title{\hwtitle}
\author{\hwauthor}
\date{\hwdate}
\maketitle

\section*{Bit Complexity of Euclid's Algorithm}
We have proved that Euclid's algorithm for computing $\gcd(a,b)$ makes at most
$O(\log a)$ iterations. What is the overall running time? Each iteration computes
$u \mod v$ for some integers. This can be done by integer division. What is its running time?
There are very sophisticated algorithms, but python probably does not come with them. 
Recall the ``school method'' for dividing integers. Have a look at the pdf slides on the 
webpage for an illustration of the school method. It is especially simple if we are dealing
with binary numbers. If $a$ and $b$ have at most $n$ bits, then the school method 
has complexity $O(n^2)$.

\begin{problem}{1}
	\statement
	Show the following, more precise bound of the school method for integer division:
	If $a$ has $n$ bits and $b$ has $k$ bits, then the school method can be implemented
	to run in $O( k(n-k))$ operations.
	\solution
	\begin{proof}
	School method for integer division will do \( n - k \) subtractions. For each subtractions, there are  \( k \) bits involved in the arithmetic.
		
	Hence the complexity of the algorithm is \( O(k(n - k))\)

	\end{proof}
\end{problem}

\begin{problem}{2}
\statement
Show that the bit complexity of Euclid's algorithm, using the school method
to compute $a \mod b$, is $O(n^2)$. That is,
if $a$ and $b$ have at most $n$ bits, then $\gcd(a,b)$ makes $O(n^2)$ bit operations.\\

In order to do so, here is python code of the Euclidean algorithm:
\begin{python}
def euclid(a,b):
	while (b > 0):
		r = a % b # so a = bu+r
		if (r == 0):
			return b
		s = b % r # so b = rv + s
		a = r
		b = s
  	return a
\end{python}
Don't be afraid to introduce notation! I recommend to let $n$ denote the number of bits of $a$.
Take some other letters for the number of bits in $b$ and so on.
\solution
\begin{proof}
	
We prove it by induction.

When \(n = 1\), it is right by check all cases.

Assume that we have proved it for all \(1 \leq n < k \), next we'll prove it for \( n=k\). By induction hypothesis, at least one of the two numbers has \(n \) bits. So we assume that the other number has \(m \) bits and we use \(T(k)\) as the complexity of the algorithm in the case of \(n=k\).

According to the conclusion we proved in Exercise 3.

\[
	\begin{split}
		T(k) &= O(m(k - m)) + T(m) \\
		&= O(m(k-m) + m^2)
	\end{split}
\]

\( m(k - m) + m^2 = O(k^2)\) for all \(m\) satisfying \(1 \leq m \leq k\).

Hence, the complexity of the algorithm is \(O(n^2)\).

\end{proof}
\end{problem}
\section*{Computing the Binomial Coefficient}

    Next, we will investigate the binomial coefficient ${n \choose k}$, which 
    you might also know by the notation $C^k_n$. The number ${n \choose k}$ is defined
    as the number of subsets of $\{1,\dots,n\}$ which have size exactly $k$. 
    This immediately shows that ${n \choose k}$ is $0$ if $k$ is negative or larger than $n$.
    You might have seen the following recurrence:
    \begin{align*}
     {n \choose k} & = {n-1 \choose k-1} + {n-1 \choose k} \textnormal{ if } n,k \geq 0 \ .
    \end{align*}



\begin{problem}{3} 
    \statement
        [A Recursive Algorithm for the Binomial Coefficient]
        Using pseudocode, write a recursive algorithm computing
        ${n \choose k}$. Implement it in python! What is 
        the running time of your algorithm, in terms of $n$ and $k$? Would you say it is an efficient
        algorithm? Why or why not?
    \solution
    \begin{python}
    	# Recursive Algorithm for the Binomial Coefficien
    	def Rec(n, k):
    	if k < 0 or k > n:
    	return 0
    	if n == 0:
    	return 1
    	return Rec(n - 1, k - 1) + Rec(n - 1, k)
    \end{python}
    
    To figure out the complexity of the recursive algorithm, what important is to figure out the number of recalculations for each sub-recursion.
    
    Notice that \( n \) always reduce by \( 1 \), and \( k \) always reduce by \( 1 \) or \( 2 \) at each recursion.
    
    Hence sub-recursion Rec \( (p, q) \) will be recalculated \( {n - p \choose k - q - n + p} \) times
    
    \textbf{lemma.}
    \({n \choose k} \leq (\frac{en}{k})^k\)
    
    TODO[finish the proof]
    
    Hence the compexity of the algorithm are as follows
    
    \[
	    \begin{split}
		    \sum_{i = 0}^{n} \sum_{j = 0}^k {n - i \choose k - j - n + i} \log{i \choose j} &= \sum_{i=0}^{n-k} \sum_{j=0}^n {n - j \choose k - n + i} \log{j \choose j - i} \\
		    &\leq \sum_{i=0}^{n-k} \sum_{j=0}^n {n - j \choose k - n + i} k\log{\frac{en}{k}} \\
		    &= \sum_{j=0}^n 2^{n-j} k \log{\frac{en}{k}} \\
		    &= O(2^{n}k \log{\frac{en}{k}}) \\
		    &= O(2^{n}k \log{\frac{n}{k}})
	    \end{split}
    \]
\end{problem}


\begin{problem}{4}
    \statement
    [A Dynamic Programming Algorithm for the Binomial Coefficient]
    Using pseudocode, write a dynamic programming algorithm
    computing ${n \choose k}$. Implement it in python! What is it running time
    in terms of $n$ and $k$?
    Would you say your algorithm is efficient? Why or why not?
    \solution
    \begin{python}
    	# Dynamic Programming Algorithm for the Binomial Coefficien
    	def Dynamic(n,k):
    	f=[[1 for i in range(0,k+1)] for j in range(0,n+1)]
    	for i in range(1, n+1):
    	for j in range(1, k+1):
    	if (i <= j):
    	f[i][j] = 1
    	else:
    	f[i][j]=f[i-1][j]+f[i-1][j-1]
    	return f[n][k]
    \end{python}
    
    [TODO]:Add runtime list of exercise 3 and 4?
    
    For calculating ${n \choose k}$, the dynamic programming algorithm runs $n \times k$ iterations.For each iteration, if f[i-1][j] and f[i-1][j-1] have at most m bits, then the running time of adding will be $O(m)$.
    
    According to the mathematical expression of ${n \choose k}=(\frac{en}{k})^k$, any element in f won't be longer then $k\log(\frac{en}{k})$ bits. If we take the upper bound of calculating f, which assumes every element in f is $k\log(\frac{en}{k})$ bits long, then the total running time will be $O(k^2n\log(\frac{n}{k}))$
    
    It's a rather better algorithm than the recursive one because it only calculate every ${n \choose k}$ once.

\end{problem}

\begin{problem}{5}
    \statement
    Suppose we are only interested in whether ${n \choose k}$ is even or odd,
  i.e., we want to compute ${n \choose k}  \mod 2$. You could do this by computing 
  ${n \choose k}$ using dynamic programming and then taking
  the result modulo $2$. What is the running time? Would you say this algorithm
  is efficient? Why or why not?
  \solution
If we first calculate ${n \choose k}$ in completely the same way of exercise 4, then the running time will still be $O(k^2nlogn)$. It isn't a good algorithm because the bit-length of answer is only 1.
 
If we are only interested in ${n \choose k}$ mod 2, then for each f[i][j] in python code of exercise 4, we can only save and calculate the last bit. So for every iteration in exercise 4, the running time is $O(1)$ because only 1 bit calculation is done. In this case, the running time will be $O(nk)$.

However, we have a more efficient way to judge whether ${n \choose k}$ is even or odd: we only need to know whether $n \& k = k$. If $n \& k = k$, then it's odd. Else it's even. It can be proved by induction as follows:

[TODO](lyq):complete the proof.
\end{problem}

\begin{problem}{6}
    \statement
    Remember the ``period'' algorithm for computing $F'_n := (F_n \mod k)$ discussed in class:
    (1) find some $i,j$ between $0$ and $k^2$ for which 
    $F'_{i} =  F'_{j}$ and $F'_{i+1} = F'_{j+1} k$. 
    Then for $d := j-i$ the sequence $F'_{n}$ will repeat every $d$ steps, as there will be a cycle.
    
    Show that a lasso cannot happen. That is, show 
    that the smallest $i$ for which this happens is $0$, i.e, for some $j$ we have
    $F'_0 = F'_j$ and $F'_1 = F'_{j+1}$ and thus $F'_n = F'_{n \textnormal{ mod }  j}$.
    \solution
    We prove this by contradiction.

    Suppose the smallest $i$ for which we have some $j$ satisfies $F'_i = F'_j$ and $F'_{i+1} = F'_{j+1}$, while $i>0$. By definition $F_{i-1}+F_i=F_{i+1}$, so $F_{i-1} \equiv  F_{i+1}-F_i(\textnormal{ mod }  k)$, and similarly $F_{j-1} \equiv  F_{j+1}-F_j(\textnormal{ mod }  k)$. Because $F_i = F_j$ and $F_{i+1} = F_{j+1}$, we have $F_{j-1} \equiv F_{i-1}(\textnormal{ mod }  k)$, then $F'_{i-1} = F'{j-1}$.

    Because $i-1$ and $j-1$ also satisfies $F'_{i-1} = F'_{j-1}$ and $F'_{(i-1)+1} = F'_{(j-1)+1}$, $i$ isn't the smallest natural number satisfying the conditions, which leads to a contradiction. And since pigeon-hole principle ensures the existence of cycle or lasso, then it can only be a cycle, i.e. for some $j$ we have $F'_0 = F'_j$ and $F'_1 = F'_{j + 1}$ thus $F'_n = F' _{n \mod j}$. So we prove that the smallest $i$ satisfying the conditions is $0$.
\end{problem}

\end{document}