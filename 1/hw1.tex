\documentclass[11pt,a4paper,oneside]{article}

\usepackage{euler,amsthm,amsmath,amsfonts,graphicx,epigraph,indentfirst,enumerate,comment,listings,fontspec,color,subcaption,listings}
\usepackage{xeCJK}
\usepackage{hw}
\usepackage{pythonhighlight}

\renewcommand{\hwtitle} {CS217 Homework 1}
\renewcommand{\hwauthor}{Akina}
\renewcommand{\hwdate}{\today}

\begin{document}
\title{\hwtitle}
\author{\hwauthor}
\date{\hwdate}
\maketitle

\section*{Bit Complexity of Euclid's Algorithm}
We have proved that Euclid's algorithm for computing $\gcd(a,b)$ makes at most
$O(\log a)$ iterations. What is the overall running time? Each iteration computes
$u \mod v$ for some integers. This can be done by integer division. What is its running time?
There are very sophisticated algorithms, but python probably does not come with them. 
Recall the ``school method'' for dividing integers. Have a look at the pdf slides on the 
webpage for an illustration of the school method. It is especially simple if we are dealing
with binary numbers. If $a$ and $b$ have at most $n$ bits, then the school method 
has complexity $O(n^2)$.

\begin{problem}{1}
	\statement
	Show the following, more precise bound of the school method for integer division:
	If $a$ has $n$ bits and $b$ has $k$ bits, then the school method can be implemented
	to run in $O( k(n-k))$ operations.
	\solution
	\begin{proof}
	School method for integer division will do $O( n - k )$ comparisons and subtractions. For each comparisons and subtractions, there are  $O(k)$ bits involved in the arithmetic. Hence the complexity of the algorithm is \( O(k(n - k))\).

	We can check our result by investigating the worst case in which the division answer like $(11\cdots1)_2$. It needs $(n - k + 1)$ fully-compared comparisons and subtractions, for each comparison it involves $k$ bits, for each subtractions it involves at most $k + 1$ bits. So the in total the bit complexity is $(n - k)(k + k + 1)$, which reaches the bound of our former result.

	\end{proof}
\end{problem}

\begin{problem}{2}
\statement
Show that the bit complexity of Euclid's algorithm, using the school method
to compute $a \mod b$, is $O(n^2)$. That is,
if $a$ and $b$ have at most $n$ bits, then $\gcd(a,b)$ makes $O(n^2)$ bit operations.\\

In order to do so, here is python code of the Euclidean algorithm:
\begin{python}
def euclid(a,b):
	while (b > 0):
		r = a % b # so a = bu+r
		if (r == 0):
			return b
		s = b % r # so b = rv + s
		a = r
		b = s
  	return a
\end{python}
Don't be afraid to introduce notation! I recommend to let $n$ denote the number of bits of $a$.
Take some other letters for the number of bits in $b$ and so on.
\solution
\begin{proof}

[Wrong Proof]

\end{proof}
\end{problem}

\section*{Computing the Binomial Coefficient}

    Next, we will investigate the binomial coefficient ${n \choose k}$, which 
    you might also know by the notation $C^k_n$. The number ${n \choose k}$ is defined
    as the number of subsets of $\{1,\dots,n\}$ which have size exactly $k$. 
    This immediately shows that ${n \choose k}$ is $0$ if $k$ is negative or larger than $n$.
    You might have seen the following recurrence:
    \begin{align*}
     {n \choose k} & = {n-1 \choose k-1} + {n-1 \choose k} \textnormal{ if } n,k \geq 0 \ .
    \end{align*}



\begin{problem}{3} 
    \statement
        [A Recursive Algorithm for the Binomial Coefficient]
        Using pseudocode, write a recursive algorithm computing
        ${n \choose k}$. Implement it in python! What is 
        the running time of your algorithm, in terms of $n$ and $k$? Would you say it is an efficient
        algorithm? Why or why not?
    \solution
    \begin{python}
# Recursive Algorithm for the Binomial Coefficien
def Rec(n, k):
    if k < 0 or k > n:
        return 0
    if n == 0:
        return 1
    return Rec(n - 1, k - 1) + Rec(n - 1, k)
    \end{python}

    To figure out the complexity of the recursive algorithm, what important is to figure out the number of recalculations for each sub-recursion.

    Notice that \( n \) always reduce by \( 1 \) at every recursion, and \( k \) always reduce by \( 1 \) or \( 2 \).

    Hence sub-recursion Rec \(p, q\) will be recalculated by \( {n - i \choose k - j - n + i} \) times

    \begin{lemma}
        \({n \choose k} = O(\frac{2^n}{\sqrt{n}})\)
    \begin{proof}
    TODO[finish the proof]
    \end{proof}
    \end{lemma}
    Hence the complexity of the algorithm are as follows

    \[
        \begin{split}
            \sum_{i = 0}^{n} \sum_{j = 0}^k {n - i \choose k - j - n + i} \log{i \choose j} &\leq \sum_{i=0}^{n} \sum_{j=0}^k \frac{2^{n - i}}{\sqrt{n-i}} \log(\frac{2^i}{\sqrt{i}}) \\
            &\leq \sum_{i=0}^{n} \sum_{j=0}^k \frac{2^{n - i}}{\sqrt{n-i}} i \\
            &\leq O(k \sqrt{n}2^n)
        \end{split}
    \]
\end{problem}


\begin{problem}{4}
    \statement
    [A Dynamic Programming Algorithm for the Binomial Coefficient]
    Using pseudocode, write a dynamic programming algorithm
    computing ${n \choose k}$. Implement it in python! What is it running time
    in terms of $n$ and $k$?
    Would you say your algorithm is efficient? Why or why not?
    \solution
    \begin{python}
#
def Dynamic(n,k):
    f=[[1 for i in range(0,k+1)] for j in range(0,n+1)]
    for i in range(1, n+1):
        for j in range(1, k+1):
            if (i <= j):
                f[i][j] = 1
            else:
                f[i][j]=f[i-1][j]+f[i-1][j-1]
    return f[n][k]
    \end{python}

    [TODO]:Add runtime list of exercise 3 and 4?

    For calculating ${n \choose k}$, the dynamic programming algorithm runs $n \times k$ iterations.For each iteration, if f[i-1][j] and f[i-1][j-1] have at most m bits, then the running time of adding will be $O(m)$.

    According to the mathematical expression of ${n \choose k}=\frac{n!}{(n-k)!k!}<n^k$, any element in f won't be longer then $klog(n)$ bits. If we take the upper bound of calculating f, which assumes every element in f is $klog(n)$ bits long, then the total running time will be $O(k^2nlogn)$

    It's a rather better algorithm than the recursive one because it only calculate every ${n \choose k}$ once.

\end{problem}

\begin{problem}{5}
    \statement
    Suppose we are only interested in whether ${n \choose k}$ is even or odd,
  i.e., we want to compute ${n \choose k}  \mod 2$. You could do this by computing 
  ${n \choose k}$ using dynamic programming and then taking
  the result modulo $2$. What is the running time? Would you say this algorithm
  is efficient? Why or why not?
  \solution
If we first calculate ${n \choose k}$ in completely the same way of exercise 4, then the running time will still be $O(k^2nlogn)$. It isn't a good algorithm because the bit-length of answer is only 1.
 
If we are only interested in ${n \choose k}$ mod 2, then for each f[i][j] in python code of exercise 4, we can only save and calculate the last bit. So for every iteration in exercise 4, the running time is $O(1)$ because only 1 bit calculation is done. In this case, the running time will be $O(nk)$.

However, we have a more efficient way to judge whether ${n \choose k}$ is even or odd: we only need to know whether $n \& k = k$. If $n \& k = k$, then it's odd. Else it's even. It can be proved by induction as follows:

[TODO](lyq):complete the proof.
\end{problem}

\begin{problem}{6}
    \statement
    Remember the ``period'' algorithm for computing $F'_n := (F_n \mod k)$ discussed in class:
    (1) find some $i,j$ between $0$ and $k^2$ for which 
    $F'_{i} =  F'_{j}$ and $F'_{i+1} = F'_{j+1} k$. 
    Then for $d := j-i$ the sequence $F'_{n}$ will repeat every $d$ steps, as there will be a cycle.
    
    Show that a lasso cannot happen. That is, show 
    that the smallest $i$ for which this happens is $0$, i.e, for some $j$ we have
    $F'_0 = F'_j$ and $F'_1 = F'_{j+1}$ and thus $F'_n = F'_{n \textnormal{ mod }  j}$.
    \solution
    We prove this by contradiction.

    Suppose the smallest $i$ for which we have some $j$ satisfies $F'_i = F'_j$ and $F'_{i+1} = F'_{j+1}$, while $i>0$. By definition $F_{i-1}+F_i=F_{i+1}$, so $F_{i-1} \equiv  F_{i+1}-F_i(\textnormal{ mod }  k)$, and similarly $F_{j-1} \equiv  F_{j+1}-F_j(\textnormal{ mod }  k)$. Because $F_i = F_j$ and $F_{i+1} = F_{j+1}$, we have $F_{j-1} \equiv F_{i-1}(\textnormal{ mod }  k)$, then $F'_{i-1} = F'{j-1}$.

    Because $i-1$ and $j-1$ also satisfies $F'_{i-1} = F'_{j-1}$ and $F'_{(i-1)+1} = F'_{(j-1)+1}$, $i$ isn't the smallest natural number satisfying the conditions, which leads to a contradiction. And since pigeon-hole principle ensures the existence of cycle or lasso, then it can only be a cycle, i.e. for some $j$ we have $F'_0 = F'_j$ and $F'_1 = F'_{j + 1}$ thus $F'_n = F' _{n \mod j}$. So we prove that the smallest $i$ satisfying the conditions is $0$.
\end{problem}

\end{document}